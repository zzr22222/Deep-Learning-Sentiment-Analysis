# -*- coding: utf-8 -*-
"""sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HdYvUTPdGjxnA1lMUCj5IqQXpyYZVhWq
"""

!pip install emoji datasets

import pandas as pd
import numpy as np
import re
import emoji
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification
import torch
from torch.utils.data import DataLoader, TensorDataset
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import copy
import random
from sklearn.metrics import accuracy_score, f1_score
import os
import requests
from tqdm import tqdm
from datasets import load_dataset
import pickle

# Text preprocessing class
class TextPreprocessor:
    def __init__(self):
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        # Common internet slang and abbreviations dictionary
        self.slang_dict = {
            "u": "you",
            "r": "are",
            "y": "why",
            "lol": "laughing out loud",
            "idk": "i do not know",
            "tbh": "to be honest",
            "imo": "in my opinion",
            "omg": "oh my god",
            "gonna": "going to",
            "wanna": "want to",
            "gimme": "give me"
        }

    def clean_text(self, text):
        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        # Remove HTML tags
        text = re.sub(r'<.*?>', '', text)
        # Process emojis (preserve sentiment information)
        text = emoji.demojize(text)
        text = re.sub(r':[a-z_]+:', lambda x: ' ' + x.group(0).replace('_', ' ').replace(':', '') + ' ', text)

        # Convert to lowercase
        text = text.lower()

        # Handle common abbreviations
        words = text.split()
        text = ' '.join(self.slang_dict.get(word, word) for word in words)

        # Handle contractions
        text = re.sub(r"won't", "will not", text)
        text = re.sub(r"can\'t", "can not", text)
        text = re.sub(r"n\'t", " not", text)
        text = re.sub(r"\'re", " are", text)
        text = re.sub(r"\'s", " is", text)
        text = re.sub(r"\'d", " would", text)
        text = re.sub(r"\'ll", " will", text)
        text = re.sub(r"\'ve", " have", text)
        text = re.sub(r"\'m", " am", text)

        # Remove special characters and extra punctuation
        text = re.sub(r'[^\w\s\']', ' ', text)
        # Remove extra spaces
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def augment_data(self, texts, labels):
        """Data augmentation method"""
        augmented_texts = []
        augmented_labels = []

        for text, label in zip(texts, labels):
            # Add original data
            augmented_texts.append(text)
            augmented_labels.append(label)

            # Augment minority class (assuming label=1 is minority)
            if label == 1:
                words = text.split()
                if len(words) > 5:  # Only process longer texts
                    # Using reverse as a simple augmentation example
                    augmented_text = ' '.join(words[::-1])
                    augmented_texts.append(augmented_text)
                    augmented_labels.append(label)

        return augmented_texts, augmented_labels

    def prepare_data(self, texts, labels, max_length=128):
        # Encode texts
        encodings = self.tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=max_length,
            return_tensors='pt'
        )

        return (
            encodings['input_ids'],
            encodings['attention_mask'],
            torch.tensor(labels)
        )

# Model training class
class SentimentClassifier:
    def __init__(self, num_labels=3):
        # Use GPU (cuda) or CPU if available
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {self.device}")

        self.model = BertForSequenceClassification.from_pretrained(
            'bert-base-uncased',
            num_labels=num_labels
        ).to(self.device)

    def train(self, train_dataloader, val_dataloader, epochs=3):
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)
        best_val_loss = float('inf')
        best_model = None

        for epoch in range(epochs):
            print(f'Epoch {epoch+1}/{epochs}:')
            progress_bar = tqdm(train_dataloader, desc='Training')
            self.model.train()
            train_loss = 0
            for batch in progress_bar:
                optimizer.zero_grad()
                input_ids = batch[0].to(self.device)
                attention_mask = batch[1].to(self.device)
                labels = batch[2].to(self.device)

                outputs = self.model(
                    input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )

                loss = outputs.loss
                train_loss += loss.item()
                loss.backward()
                optimizer.step()
                progress_bar.set_postfix({'loss': loss.item()})

            # Validation
            val_loss, val_predictions, val_labels = self.evaluate(val_dataloader)
            val_metrics = self.calculate_metrics(val_labels, val_predictions)

            print(f'Epoch {epoch+1}:')
            print(f'Average training loss: {train_loss/len(train_dataloader)}')
            print(f'Validation metrics: {val_metrics}')

            # Save best model
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_model = copy.deepcopy(self.model.state_dict())

        # Load best model
        if best_model:
            self.model.load_state_dict(best_model)

    def evaluate(self, dataloader):
        self.model.eval()
        total_loss = 0
        predictions = []
        actual_labels = []

        with torch.no_grad():
            for batch in dataloader:
                input_ids = batch[0].to(self.device)
                attention_mask = batch[1].to(self.device)
                labels = batch[2].to(self.device)

                outputs = self.model(
                    input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )

                total_loss += outputs.loss.item()
                predictions.extend(outputs.logits.argmax(dim=-1).cpu().numpy())
                actual_labels.extend(labels.cpu().numpy())

        return total_loss, predictions, actual_labels

    def calculate_metrics(self, true_labels, predictions):
        """Calculate various evaluation metrics"""
        return {
            'accuracy': accuracy_score(true_labels, predictions),
            'macro_f1': f1_score(true_labels, predictions, average='macro'),
            'weighted_f1': f1_score(true_labels, predictions, average='weighted'),
            'per_class_f1': f1_score(true_labels, predictions, average=None)
        }

    def test_robustness(self, text, n_augmentations=5):
        """Test model robustness to text variations"""
        predictions = []
        augmented_texts = self.create_text_variations(text, n_augmentations)

        for aug_text in augmented_texts:
            inputs = self.tokenizer(
                aug_text,
                return_tensors='pt',
                truncation=True,
                padding=True
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)
                pred = outputs.logits.argmax(dim=-1).item()
                predictions.append(pred)

        from statistics import mode
        main_prediction = mode(predictions)
        consistency = predictions.count(main_prediction) / len(predictions)

        return main_prediction, consistency

    def create_text_variations(self, text, n_variations):
        """Create text variations for robustness testing"""
        variations = [text]

        # 1. Add random characters
        text_with_noise = ''.join([c + ' ' if random.random() < 0.1 else c for c in text])
        variations.append(text_with_noise)

        # 2. Random case transformation
        text_case = ''.join([c.upper() if random.random() < 0.3 else c for c in text])
        variations.append(text_case)

        # 3. Add spelling errors
        words = text.split()
        misspelled = []
        for word in words:
            if len(word) > 3 and random.random() < 0.2:
                pos = random.randint(0, len(word)-1)
                word = word[:pos] + word[pos+1:]
            misspelled.append(word)
        variations.append(' '.join(misspelled))

        return variations[:n_variations]

# Visualization class
class VisualizationTools:
    @staticmethod
    def plot_confusion_matrix(y_true, y_pred, labels):
        cm = confusion_matrix(y_true, y_pred)
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=labels, yticklabels=labels)
        plt.xlabel('Predicted')
        plt.ylabel('True')
        plt.title('Confusion Matrix')
        plt.show()

    @staticmethod
    def plot_training_history(history):
        plt.figure(figsize=(12, 4))

        plt.subplot(1, 2, 1)
        plt.plot(history['train_loss'], label='Training Loss')
        plt.plot(history['val_loss'], label='Validation Loss')
        plt.title('Loss Over Time')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()

        plt.subplot(1, 2, 2)
        plt.plot(history['train_acc'], label='Training Accuracy')
        plt.plot(history['val_acc'], label='Validation Accuracy')
        plt.title('Accuracy Over Time')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()

        plt.tight_layout()
        plt.show()

    @staticmethod
    def generate_wordcloud(texts, labels, title="Word Cloud"):
        """Generate word clouds for different sentiments"""
        from wordcloud import WordCloud
        import jieba
        fig, axes = plt.subplots(1, 3, figsize=(20, 6))
        sentiment_names = ['Negative', 'Neutral', 'Positive']

        for idx, sentiment in enumerate([0, 1, 2]):
            sentiment_texts = ' '.join([text for text, label in zip(texts, labels) if label == sentiment])

            wordcloud = WordCloud(
                width=800, height=400,
                background_color='white',
                max_words=100
            )

            wordcloud.generate(sentiment_texts)
            axes[idx].imshow(wordcloud, interpolation='bilinear')
            axes[idx].axis('off')
            axes[idx].set_title(f'{sentiment_names[idx]} sentiment word cloud')

        plt.tight_layout()
        plt.show()

    @staticmethod
    def plot_sentiment_distribution(labels):
        """Plot sentiment distribution pie chart"""
        plt.figure(figsize=(10, 8))
        sentiment_counts = pd.Series(labels).value_counts()
        plt.pie(
            sentiment_counts.values,
            labels=['Negative', 'Neutral', 'Positive'],
            autopct='%1.1f%%',
            colors=['#FF9999', '#66B2FF', '#99FF99']
        )
        plt.title('Sentiment Distribution')
        plt.show()

    @staticmethod
    def plot_text_length_distribution(texts, labels):
        """Plot text length distribution for each sentiment"""
        lengths = [len(text.split()) for text in texts]
        sentiment_df = pd.DataFrame({
            'Text Length': lengths,
            'Sentiment': ['Negative' if l == 0 else 'Neutral' if l == 1 else 'Positive' for l in labels]
        })

        plt.figure(figsize=(12, 6))
        sns.boxplot(x='Sentiment', y='Text Length', data=sentiment_df)
        plt.title('Text Length Distribution by Sentiment')
        plt.show()

    @staticmethod
    def plot_top_words(texts, labels, top_n=20):
        """Plot the most common words for each sentiment"""
        from collections import Counter
        import jieba

        plt.figure(figsize=(15, 12))
        sentiment_names = ['Negative', 'Neutral', 'Positive']

        for idx, sentiment in enumerate([0, 1, 2]):
            sentiment_texts = ' '.join([text for text, label in zip(texts, labels) if label == sentiment])

            words = jieba.cut(sentiment_texts)
            word_counts = Counter(words)

            top_words = dict(sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:top_n])

            plt.subplot(3, 1, idx+1)
            plt.barh(list(top_words.keys()), list(top_words.values()))
            plt.title(f'{sentiment_names[idx]} sentiment most common words')

        plt.tight_layout()
        plt.show()

def download_datasets():
    """Download and prepare datasets"""
    if not os.path.exists('data'):
        os.makedirs('data')

    if not os.path.exists('data/twitter_sentiment.csv'):
        print("Downloading tweet_eval sentiment dataset...")
        dataset = load_dataset('tweet_eval', 'sentiment')

        train_df = pd.DataFrame({
            'text': dataset['train']['text'],
            'sentiment': dataset['train']['label']
        })
        test_df = pd.DataFrame({
            'text': dataset['test']['text'],
            'sentiment': dataset['test']['label']
        })
        val_df = pd.DataFrame({
            'text': dataset['validation']['text'],
            'sentiment': dataset['validation']['label']
        })

        df = pd.concat([train_df, test_df, val_df], ignore_index=True)
        df.to_csv('data/twitter_sentiment.csv', index=False)

    if not os.path.exists('data/imdb_reviews.csv'):
        print("Downloading IMDB review dataset...")
        dataset = load_dataset('imdb')

        train_df = pd.DataFrame({
            'review': dataset['train']['text'],
            'sentiment': dataset['train']['label']
        })
        test_df = pd.DataFrame({
            'review': dataset['test']['text'],
            'sentiment': dataset['test']['label']
        })

        df = pd.concat([train_df, test_df], ignore_index=True)
        df.to_csv('data/imdb_reviews.csv', index=False)

def load_and_prepare_data():
    """Load and preprocess data"""
    download_datasets()

    twitter_data = pd.read_csv('data/twitter_sentiment.csv')
    twitter_texts = twitter_data['text'].tolist()
    twitter_labels = twitter_data['sentiment'].tolist()

    imdb_data = pd.read_csv('data/imdb_reviews.csv')
    imdb_texts = imdb_data['review'].tolist()
    imdb_labels = [2 if label == 1 else 0 for label in imdb_data['sentiment']]

    preprocessor = TextPreprocessor()

    print("Cleaning Twitter texts...")
    twitter_texts = [preprocessor.clean_text(text) for text in tqdm(twitter_texts)]
    print("Cleaning IMDB texts...")
    imdb_texts = [preprocessor.clean_text(text) for text in tqdm(imdb_texts)]

    texts = twitter_texts + imdb_texts
    labels = twitter_labels + imdb_labels

    # print("Performing data augmentation...")
    texts, labels = preprocessor.augment_data(texts, labels)

    label_dist = pd.Series(labels).value_counts()
    print("\nLabel distribution:")
    print(label_dist)

    return texts, labels

def train_and_evaluate():
    """Function to handle data loading, preprocessing, training and evaluation"""
    if torch.cuda.is_available():
        print("CUDA GPU is available. Using GPU for training.")
    else:
        print("CUDA not available, will use CPU training.")

    texts, labels = load_and_prepare_data()
    preprocessor = TextPreprocessor()
    print("Cleaning text data...")
    cleaned_texts = [preprocessor.clean_text(text) for text in texts]

    X_train, X_test, y_train, y_test = train_test_split(
        cleaned_texts, labels, test_size=0.2, random_state=42, stratify=labels
    )

    print(f"Training set size: {len(X_train)}")
    print(f"Test set size: {len(X_test)}")

    train_inputs = preprocessor.prepare_data(X_train, y_train)
    test_inputs = preprocessor.prepare_data(X_test, y_test)

    train_dataset = TensorDataset(*train_inputs)
    test_dataset = TensorDataset(*test_inputs)

    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_dataloader = DataLoader(test_dataset, batch_size=32)

    classifier = SentimentClassifier()
    classifier.train(train_dataloader, test_dataloader)

    _, predictions, actual_labels = classifier.evaluate(test_dataloader)

    print(classification_report(
        actual_labels,
        predictions,
        target_names=['Negative', 'Neutral', 'Positive']
    ))

    # Save results and model
    results = {
        'actual_labels': actual_labels,
        'predictions': predictions,
        'cleaned_texts': cleaned_texts,
        'labels': labels
    }

    with open('sentiment_results.pkl', 'wb') as f:
        pickle.dump(results, f)

    torch.save(classifier.model.state_dict(), 'sentiment_model.pth')

    return cleaned_texts, labels, actual_labels, predictions

def visualize_results(cleaned_texts, labels, actual_labels, predictions):
    """Function to visualize the results after training and evaluation"""
    viz = VisualizationTools()
    # Confusion Matrix
    viz.plot_confusion_matrix(actual_labels, predictions, ['Negative', 'Neutral', 'Positive'])
    # Word Cloud
    viz.generate_wordcloud(cleaned_texts, labels)
    # Sentiment Distribution
    viz.plot_sentiment_distribution(labels)
    # Text Length Distribution
    viz.plot_text_length_distribution(cleaned_texts, labels)
    # Most Common Words
    viz.plot_top_words(cleaned_texts, labels)

cleaned_texts, labels, actual_labels, predictions = train_and_evaluate()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

def run_baseline_classic_nlp(cleaned_texts, labels):
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        cleaned_texts, labels, test_size=0.2, random_state=42, stratify=labels
    )

    # Vectorize the text using TF-IDF
    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
    X_train_vec = vectorizer.fit_transform(X_train)
    X_test_vec = vectorizer.transform(X_test)

    # Train a logistic regression model
    clf = LogisticRegression(max_iter=1000, class_weight='balanced')
    clf.fit(X_train_vec, y_train)

    # Predict
    y_pred = clf.predict(X_test_vec)

    # Print the classification report as baseline
    print("Classic NLP Baseline Results:")
    print(classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive']))

# 示例使用（在main中已完成数据清理后可调用）
if __name__ == "__main__":
    texts, labels = load_and_prepare_data()  # 已有的函数
    preprocessor = TextPreprocessor()
    cleaned_texts = [preprocessor.clean_text(text) for text in texts]

    # 运行传统NLP baseline
    run_baseline_classic_nlp(cleaned_texts, labels)

    # # 然后再运行BERT模型训练以进行对照
    # cleaned_texts, labels, actual_labels, predictions = train_and_evaluate()
    # visualize_results(cleaned_texts, labels, actual_labels, predictions)

visualize_results(cleaned_texts, labels, actual_labels, predictions)

# 加载结果
    with open('sentiment_results.pkl', 'rb') as f:
        results = pickle.load(f)

    actual_labels = results['actual_labels']
    predictions = results['predictions']
    cleaned_texts = results['cleaned_texts']
    labels = results['labels']